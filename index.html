<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Agent Dashboard - Real-time Audio Streaming</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          "Helvetica Neue", Arial, sans-serif;
        background: #0a0e27;
        background-image: radial-gradient(
            at 0% 0%,
            rgba(59, 130, 246, 0.1) 0px,
            transparent 50%
          ),
          radial-gradient(
            at 100% 0%,
            rgba(139, 92, 246, 0.1) 0px,
            transparent 50%
          ),
          radial-gradient(
            at 100% 100%,
            rgba(59, 130, 246, 0.1) 0px,
            transparent 50%
          );
        min-height: 100vh;
        padding: 20px;
        color: #e5e7eb;
      }

      .dashboard {
        max-width: 1400px;
        margin: 0 auto;
      }

      .dashboard-header {
        text-align: center;
        margin-bottom: 30px;
        padding: 20px;
        background: rgba(17, 24, 39, 0.6);
        backdrop-filter: blur(10px);
        border-radius: 16px;
        border: 1px solid rgba(59, 130, 246, 0.2);
      }

      h1 {
        color: #ffffff;
        margin-bottom: 10px;
        font-size: 32px;
        font-weight: 700;
        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .subtitle {
        color: #9ca3af;
        margin-bottom: 0;
        font-size: 14px;
        font-weight: 400;
      }

      /* Grid Layout */
      .dashboard-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        grid-template-rows: auto;
        gap: 20px;
        margin-bottom: 20px;
      }

      .left-column {
        display: flex;
        flex-direction: column;
        gap: 20px;
      }

      .right-column {
        display: flex;
        flex-direction: column;
        gap: 20px;
        max-height: calc(100vh - 280px);
        overflow-y: auto;
        padding-right: 8px;
      }

      .right-column::-webkit-scrollbar {
        width: 8px;
      }

      .right-column::-webkit-scrollbar-track {
        background: rgba(0, 0, 0, 0.2);
        border-radius: 4px;
      }

      .right-column::-webkit-scrollbar-thumb {
        background: rgba(59, 130, 246, 0.5);
        border-radius: 4px;
      }

      .right-column::-webkit-scrollbar-thumb:hover {
        background: rgba(59, 130, 246, 0.7);
      }

      @media (max-width: 1024px) {
        .dashboard-grid {
          grid-template-columns: 1fr;
        }

        .right-column {
          max-height: none;
          overflow-y: visible;
        }
      }

      /* Card Styles */
      .card {
        background: rgba(17, 24, 39, 0.6);
        backdrop-filter: blur(10px);
        border-radius: 16px;
        padding: 24px;
        border: 1px solid rgba(59, 130, 246, 0.2);
        transition: all 0.3s ease;
      }

      .card:hover {
        border-color: rgba(59, 130, 246, 0.4);
        box-shadow: 0 10px 40px rgba(59, 130, 246, 0.1);
      }

      .card-title {
        color: #ffffff;
        font-size: 18px;
        font-weight: 600;
        margin-bottom: 16px;
        display: flex;
        align-items: center;
        gap: 8px;
      }

      .card-title .icon {
        font-size: 20px;
      }

      /* Input Groups */
      .input-group {
        margin-bottom: 16px;
      }

      label {
        display: block;
        margin-bottom: 8px;
        color: #9ca3af;
        font-weight: 500;
        font-size: 14px;
      }

      input[type="text"],
      select {
        width: 100%;
        padding: 12px 16px;
        background: rgba(17, 24, 39, 0.8);
        border: 1px solid rgba(75, 85, 99, 0.5);
        border-radius: 8px;
        font-size: 14px;
        color: #e5e7eb;
        transition: all 0.3s;
      }

      input[type="text"]:focus,
      select:focus {
        outline: none;
        border-color: #3b82f6;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
      }

      select {
        cursor: pointer;
      }

      select option {
        background: #1f2937;
        color: #e5e7eb;
      }

      /* Button Styles */
      .button-group {
        display: flex;
        gap: 12px;
        margin-bottom: 0;
      }

      button {
        flex: 1;
        padding: 14px 20px;
        font-size: 15px;
        font-weight: 600;
        cursor: pointer;
        border: none;
        border-radius: 10px;
        transition: all 0.3s;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 8px;
        color: #ffffff;
        position: relative;
        overflow: hidden;
      }

      button:before {
        content: "";
        position: absolute;
        top: 50%;
        left: 50%;
        width: 0;
        height: 0;
        border-radius: 50%;
        background: rgba(255, 255, 255, 0.1);
        transform: translate(-50%, -50%);
        transition: width 0.6s, height 0.6s;
      }

      button:hover:before {
        width: 300px;
        height: 300px;
      }

      button span {
        position: relative;
        z-index: 1;
      }

      .btn-connect {
        background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%);
        box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
      }

      .btn-connect:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(59, 130, 246, 0.4);
      }

      .btn-start {
        background: linear-gradient(135deg, #10b981 0%, #059669 100%);
        box-shadow: 0 4px 12px rgba(16, 185, 129, 0.3);
      }

      .btn-start:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(16, 185, 129, 0.4);
      }

      .btn-stop {
        background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
        box-shadow: 0 4px 12px rgba(239, 68, 68, 0.3);
      }

      .btn-stop:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(239, 68, 68, 0.4);
      }

      button:disabled {
        background: linear-gradient(135deg, #374151 0%, #1f2937 100%);
        cursor: not-allowed;
        opacity: 0.5;
        box-shadow: none;
      }

      button:disabled:hover {
        transform: none;
      }

      /* Status & Stats */
      .status-grid {
        display: grid;
        grid-template-columns: 1fr;
        gap: 12px;
        margin-bottom: 20px;
      }

      @media (min-width: 1400px) {
        .status-grid {
          grid-template-columns: repeat(3, 1fr);
        }
      }

      .status-item {
        background: rgba(59, 130, 246, 0.05);
        padding: 16px;
        border-radius: 10px;
        border: 1px solid rgba(59, 130, 246, 0.1);
      }

      .status-label {
        color: #9ca3af;
        font-size: 12px;
        font-weight: 500;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-bottom: 8px;
        display: block;
      }

      .status-value {
        color: #ffffff;
        font-size: 18px;
        font-weight: 700;
        display: flex;
        align-items: center;
        gap: 6px;
      }

      .status-connected {
        color: #10b981;
      }

      .status-disconnected {
        color: #6b7280;
      }

      .status-recording {
        color: #f59e0b;
        animation: pulse 1.5s infinite;
      }

      @keyframes pulse {
        0%,
        100% {
          opacity: 1;
        }
        50% {
          opacity: 0.5;
        }
      }

      .stats {
        display: grid;
        grid-template-columns: repeat(3, 1fr);
        gap: 12px;
      }

      .stat-box {
        background: rgba(59, 130, 246, 0.05);
        padding: 20px;
        border-radius: 10px;
        text-align: center;
        border: 1px solid rgba(59, 130, 246, 0.1);
        transition: all 0.3s ease;
      }

      .stat-box:hover {
        border-color: rgba(59, 130, 246, 0.3);
        transform: translateY(-2px);
      }

      .stat-value {
        font-size: 28px;
        font-weight: 700;
        color: #3b82f6;
        margin-bottom: 8px;
      }

      .stat-label {
        font-size: 12px;
        color: #9ca3af;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        font-weight: 500;
      }

      /* Conversation Boxes */
      .conversation-box {
        background: rgba(17, 24, 39, 0.6);
        border-radius: 12px;
        padding: 20px;
        margin-bottom: 16px;
        border: 1px solid rgba(75, 85, 99, 0.3);
        min-height: 100px;
        transition: all 0.3s ease;
      }

      .conversation-box.hidden {
        display: none;
      }

      .box-header {
        display: flex;
        align-items: center;
        gap: 10px;
        margin-bottom: 12px;
        font-size: 16px;
        font-weight: 600;
        color: #ffffff;
      }

      .box-header .icon {
        font-size: 20px;
      }

      .box-content {
        color: #d1d5db;
        font-size: 16px;
        line-height: 1.6;
        min-height: 40px;
      }

      .user-transcript {
        border-left: 3px solid #3b82f6;
      }

      .user-transcript .box-header {
        color: #3b82f6;
      }

      .ai-response {
        border-left: 3px solid #10b981;
      }

      .ai-response .box-header {
        color: #10b981;
      }

      .box-content.thinking {
        font-style: italic;
        color: #9ca3af;
      }

      /* Latency Metrics */
      .latency-grid {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        gap: 12px;
      }

      @media (min-width: 1400px) {
        .latency-grid {
          grid-template-columns: repeat(4, 1fr);
        }
      }

      .latency-item {
        background: rgba(249, 115, 22, 0.05);
        padding: 16px;
        border-radius: 10px;
        text-align: center;
        border: 1px solid rgba(249, 115, 22, 0.2);
      }

      .latency-label {
        color: #9ca3af;
        font-size: 12px;
        font-weight: 500;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-bottom: 8px;
        display: block;
      }

      .latency-value {
        color: #f97316;
        font-size: 24px;
        font-weight: 700;
      }

      /* Process Flow Indicator */
      .process-flow {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 20px;
        background: rgba(17, 24, 39, 0.4);
        border-radius: 12px;
        margin-bottom: 20px;
      }

      .flow-step {
        flex: 1;
        text-align: center;
        position: relative;
        padding: 16px;
        background: rgba(59, 130, 246, 0.05);
        border-radius: 10px;
        border: 2px solid rgba(75, 85, 99, 0.3);
        transition: all 0.3s ease;
      }

      .flow-step.active {
        border-color: #3b82f6;
        background: rgba(59, 130, 246, 0.1);
        box-shadow: 0 0 20px rgba(59, 130, 246, 0.3);
      }

      .flow-step .icon {
        font-size: 24px;
        margin-bottom: 8px;
      }

      .flow-step .label {
        font-size: 12px;
        font-weight: 600;
        color: #9ca3af;
        text-transform: uppercase;
        letter-spacing: 0.5px;
      }

      .flow-step.active .label {
        color: #3b82f6;
      }

      .flow-arrow {
        font-size: 20px;
        color: #4b5563;
        margin: 0 8px;
      }

      /* Console/Log Section */
      #messages {
        background: rgba(0, 0, 0, 0.5);
        color: #d4d4d4;
        padding: 20px;
        border-radius: 12px;
        max-height: 300px;
        overflow-y: auto;
        font-family: "SF Mono", "Monaco", "Inconsolata", "Fira Code",
          "Courier New", monospace;
        font-size: 13px;
        line-height: 1.6;
        border: 1px solid rgba(75, 85, 99, 0.3);
      }

      #messages::-webkit-scrollbar {
        width: 8px;
      }

      #messages::-webkit-scrollbar-track {
        background: rgba(0, 0, 0, 0.2);
        border-radius: 4px;
      }

      #messages::-webkit-scrollbar-thumb {
        background: rgba(59, 130, 246, 0.5);
        border-radius: 4px;
      }

      #messages::-webkit-scrollbar-thumb:hover {
        background: rgba(59, 130, 246, 0.7);
      }

      .message {
        padding: 6px 0;
        border-bottom: 1px solid rgba(75, 85, 99, 0.2);
      }

      .message:last-child {
        border-bottom: none;
      }

      .timestamp {
        color: #6b7280;
        margin-right: 10px;
        font-weight: 500;
      }

      .msg-info {
        color: #60a5fa;
      }
      .msg-success {
        color: #34d399;
      }
      .msg-error {
        color: #f87171;
      }
      .msg-warning {
        color: #fbbf24;
      }

      /* Audio Status Badge */
      .audio-badge {
        display: inline-flex;
        align-items: center;
        gap: 6px;
        padding: 6px 12px;
        background: rgba(16, 185, 129, 0.1);
        border: 1px solid rgba(16, 185, 129, 0.3);
        border-radius: 20px;
        font-size: 12px;
        font-weight: 600;
        color: #10b981;
      }

      /* Responsive Design */
      @media (max-width: 768px) {
        .status-grid,
        .stats,
        .latency-grid {
          grid-template-columns: 1fr;
        }

        .process-flow {
          flex-direction: column;
          gap: 12px;
        }

        .flow-arrow {
          transform: rotate(90deg);
        }
      }
    </style>
  </head>
  <body>
    <div class="dashboard">
      <div class="dashboard-header">
        <h1>üé§ Real-time Voice Agent Dashboard</h1>
        <p class="subtitle">
          Interactive voice streaming with AI-powered speech recognition and
          response
        </p>
      </div>

      <!-- Process Flow Visualization -->
      <div class="process-flow">
        <div class="flow-step" id="flowConnect">
          <div class="icon">üîå</div>
          <div class="label">Connect</div>
        </div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-step" id="flowListen">
          <div class="icon">üëÇ</div>
          <div class="label">Listen (STT)</div>
        </div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-step" id="flowThink">
          <div class="icon">ü§ñ</div>
          <div class="label">Think (LLM)</div>
        </div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-step" id="flowSpeak">
          <div class="icon">üîä</div>
          <div class="label">Speak (TTS)</div>
        </div>
      </div>

      <div class="dashboard-grid">
        <!-- LEFT COLUMN: Controls and Metrics -->
        <div class="left-column">
          <!-- Connection Settings Card -->
          <div class="card">
            <h3 class="card-title">
              <span class="icon">‚öôÔ∏è</span>
              Connection Settings
            </h3>
            <div class="input-group">
              <label for="protocol">Protocol:</label>
              <select id="protocol">
                <option value="websocket">WebSocket (Traditional)</option>
                <option value="webrtc">WebRTC (Low Latency)</option>
              </select>
            </div>

            <div class="input-group" id="wsUrlGroup">
              <label for="wsUrl">WebSocket Server URL:</label>
              <input
                type="text"
                id="wsUrl"
                value="ws://localhost:8000/ws/audio"
                placeholder="ws://localhost:8000/ws/audio"
              />
            </div>

            <div class="input-group" id="webrtcUrlGroup" style="display: none">
              <label for="webrtcUrl">WebRTC Server URL:</label>
              <input
                type="text"
                id="webrtcUrl"
                value="http://localhost:8000"
                placeholder="http://localhost:8000"
              />
            </div>

            <div class="button-group">
              <button id="connectBtn" class="btn-connect">
                <span>üîå Connect</span>
              </button>
              <button id="disconnectBtn" class="btn-stop" disabled>
                <span>‚ùå Disconnect</span>
              </button>
            </div>
          </div>

          <!-- System Status Card -->
          <div class="card">
            <h3 class="card-title">
              <span class="icon">üìä</span>
              System Status
            </h3>
            <div class="status-grid">
              <div class="status-item">
                <span class="status-label">Connection</span>
                <span
                  id="connectionStatus"
                  class="status-value status-disconnected"
                >
                  Disconnected
                </span>
              </div>
              <div class="status-item">
                <span class="status-label">Recording</span>
                <span id="recordingStatus" class="status-value">Idle</span>
              </div>
              <div class="status-item">
                <span class="status-label">Protocol</span>
                <span id="activeProtocol" class="status-value">None</span>
              </div>
            </div>
          </div>

          <!-- Recording Controls Card -->
          <div class="card">
            <h3 class="card-title">
              <span class="icon">üéôÔ∏è</span>
              Recording Controls
            </h3>
            <div class="button-group">
              <button id="startBtn" class="btn-start" disabled>
                <span>üéôÔ∏è Start Recording</span>
              </button>
              <button id="stopBtn" class="btn-stop" disabled>
                <span>‚èπÔ∏è Stop Recording</span>
              </button>
            </div>
            <div class="button-group" style="margin-top: 12px">
              <button id="testAudioBtn" class="btn-connect">
                <span>üîä Test Audio</span>
              </button>
            </div>
          </div>

          <!-- Statistics Card -->
          <div class="card">
            <h3 class="card-title">
              <span class="icon">üìà</span>
              Stream Statistics
            </h3>
            <div class="stats">
              <div class="stat-box">
                <div class="stat-value" id="chunksCount">0</div>
                <div class="stat-label">Chunks Sent</div>
              </div>
              <div class="stat-box">
                <div class="stat-value" id="bytesCount">0</div>
                <div class="stat-label">Bytes Sent</div>
              </div>
              <div class="stat-box">
                <div class="stat-value" id="duration">0s</div>
                <div class="stat-label">Duration</div>
              </div>
            </div>
          </div>

          <!-- Latency Metrics Card -->
          <div class="card" id="latencyCard" style="display: none">
            <h3 class="card-title">
              <span class="icon">‚ö°</span>
              Performance Metrics
            </h3>
            <div class="latency-grid">
              <div class="latency-item">
                <span class="latency-label">STT</span>
                <div class="latency-value" id="sttLatency">--</div>
              </div>
              <div class="latency-item">
                <span class="latency-label">LLM</span>
                <div class="latency-value" id="llmLatency">--</div>
              </div>
              <div class="latency-item">
                <span class="latency-label">TTS</span>
                <div class="latency-value" id="ttsLatency">--</div>
              </div>
              <div class="latency-item">
                <span class="latency-label">Total</span>
                <div class="latency-value" id="totalLatency">--</div>
              </div>
            </div>
          </div>
        </div>

        <!-- RIGHT COLUMN: Conversation and Console -->
        <div class="right-column">
          <!-- Conversation Card -->
          <div class="card">
            <h3 class="card-title">
              <span class="icon">üí¨</span>
              Conversation
            </h3>

            <div
              id="transcriptBox"
              class="conversation-box user-transcript hidden"
            >
              <div class="box-header">
                <span class="icon">üë§</span>
                <span>You're saying...</span>
              </div>
              <div id="transcriptContent" class="box-content thinking">
                Waiting for speech...
              </div>
            </div>

            <div
              id="llmResponseBox"
              class="conversation-box ai-response hidden"
            >
              <div class="box-header">
                <span class="icon">ü§ñ</span>
                <span>AI Response</span>
                <span
                  id="audioStatus"
                  class="audio-badge"
                  style="display: none"
                ></span>
              </div>
              <div id="llmResponseContent" class="box-content thinking">
                Thinking...
              </div>
            </div>
          </div>

          <!-- System Console Card -->
          <div class="card">
            <h3 class="card-title">
              <span class="icon">üñ•Ô∏è</span>
              System Console
            </h3>
            <div id="messages"></div>
          </div>
        </div>
      </div>
    </div>

    <script>
      let websocket = null;
      let mediaRecorder = null;
      let audioStream = null;
      let isRecording = false;
      let chunksCount = 0;
      let bytesCount = 0;
      let startTime = null;
      let durationInterval = null;

      // WebRTC variables
      let peerConnection = null;
      let webrtcSignalingWs = null;
      let currentProtocol = "websocket";

      // DOM elements
      const protocolSelect = document.getElementById("protocol");
      const wsUrlInput = document.getElementById("wsUrl");
      const webrtcUrlInput = document.getElementById("webrtcUrl");
      const wsUrlGroup = document.getElementById("wsUrlGroup");
      const webrtcUrlGroup = document.getElementById("webrtcUrlGroup");
      const connectBtn = document.getElementById("connectBtn");
      const disconnectBtn = document.getElementById("disconnectBtn");
      const startBtn = document.getElementById("startBtn");
      const stopBtn = document.getElementById("stopBtn");
      const connectionStatus = document.getElementById("connectionStatus");
      const recordingStatus = document.getElementById("recordingStatus");
      const activeProtocol = document.getElementById("activeProtocol");
      const messagesDiv = document.getElementById("messages");
      const chunksCountEl = document.getElementById("chunksCount");
      const bytesCountEl = document.getElementById("bytesCount");
      const durationEl = document.getElementById("duration");
      const transcriptBox = document.getElementById("transcriptBox");
      const transcriptContent = document.getElementById("transcriptContent");
      const llmResponseBox = document.getElementById("llmResponseBox");
      const llmResponseContent = document.getElementById("llmResponseContent");
      const audioStatus = document.getElementById("audioStatus");
      const testAudioBtn = document.getElementById("testAudioBtn");
      const latencyCard = document.getElementById("latencyCard");
      const sttLatencyEl = document.getElementById("sttLatency");
      const llmLatencyEl = document.getElementById("llmLatency");
      const ttsLatencyEl = document.getElementById("ttsLatency");
      const totalLatencyEl = document.getElementById("totalLatency");

      // Process flow indicators
      const flowConnect = document.getElementById("flowConnect");
      const flowListen = document.getElementById("flowListen");
      const flowThink = document.getElementById("flowThink");
      const flowSpeak = document.getElementById("flowSpeak");

      // Audio playback setup
      let audioContext = null;
      let audioQueue = [];
      let isPlayingAudio = false;
      let nextPlayTime = 0;
      let allAudioChunks = [];

      // Update process flow visualization
      function updateProcessFlow(step) {
        // Reset all steps
        flowConnect.classList.remove("active");
        flowListen.classList.remove("active");
        flowThink.classList.remove("active");
        flowSpeak.classList.remove("active");

        // Activate current step
        switch (step) {
          case "connect":
            flowConnect.classList.add("active");
            break;
          case "listen":
            flowListen.classList.add("active");
            break;
          case "think":
            flowThink.classList.add("active");
            break;
          case "speak":
            flowSpeak.classList.add("active");
            break;
          case "none":
          default:
            // No step active
            break;
        }
      }

      // Initialize Web Audio API
      function initAudioContext() {
        if (!audioContext) {
          audioContext = new (window.AudioContext ||
            window.webkitAudioContext)();
          nextPlayTime = audioContext.currentTime;
        }
      }

      // Protocol selector handler
      protocolSelect.addEventListener("change", (e) => {
        currentProtocol = e.target.value;
        if (currentProtocol === "websocket") {
          wsUrlGroup.style.display = "block";
          webrtcUrlGroup.style.display = "none";
        } else {
          wsUrlGroup.style.display = "none";
          webrtcUrlGroup.style.display = "block";
        }
        addMessage(
          `üì° Protocol switched to: ${currentProtocol.toUpperCase()}`,
          "info"
        );
      });

      // Update latency metrics display
      function updateLatencyMetrics(metrics) {
        latencyCard.style.display = "block";

        if (metrics.stt_latency !== undefined) {
          sttLatencyEl.textContent = `${metrics.stt_latency.toFixed(0)}ms`;
        }
        if (metrics.llm_latency !== undefined) {
          llmLatencyEl.textContent = `${metrics.llm_latency.toFixed(0)}ms`;
        }
        if (metrics.tts_latency !== undefined) {
          ttsLatencyEl.textContent = `${metrics.tts_latency.toFixed(0)}ms`;
        }

        // Calculate total
        const total =
          (metrics.stt_latency || 0) +
          (metrics.llm_latency || 0) +
          (metrics.tts_latency || 0);
        if (total > 0) {
          totalLatencyEl.textContent = `${total.toFixed(0)}ms`;
        }
      }

      // Play audio chunk
      async function playAudioChunk(base64Audio) {
        try {
          console.log(
            "üîä Attempting to play audio chunk, length:",
            base64Audio.length
          );

          initAudioContext();

          // Resume audio context if suspended (browser security)
          if (audioContext.state === "suspended") {
            console.log("üîì Resuming audio context...");
            await audioContext.resume();
          }

          // Decode base64 to binary
          const binaryString = atob(base64Audio);
          const bytes = new Uint8Array(binaryString.length);
          for (let i = 0; i < binaryString.length; i++) {
            bytes[i] = binaryString.charCodeAt(i);
          }

          console.log("üì¶ Decoded audio bytes:", bytes.length);

          // Convert PCM linear16 to Float32Array for Web Audio API
          const pcmData = new Int16Array(bytes.buffer);
          const floatData = new Float32Array(pcmData.length);

          // Convert 16-bit PCM to float [-1.0, 1.0]
          for (let i = 0; i < pcmData.length; i++) {
            floatData[i] = pcmData[i] / 32768.0;
          }

          console.log(
            "üéµ PCM samples:",
            floatData.length,
            "Duration:",
            floatData.length / 24000,
            "seconds"
          );

          // Create audio buffer (mono, 24kHz - matches backend)
          const audioBuffer = audioContext.createBuffer(
            1,
            floatData.length,
            24000
          );
          audioBuffer.getChannelData(0).set(floatData);

          // Create buffer source
          const source = audioContext.createBufferSource();
          source.buffer = audioBuffer;

          // Add gain node for volume control
          const gainNode = audioContext.createGain();
          gainNode.gain.value = 1.0; // Full volume

          source.connect(gainNode);
          gainNode.connect(audioContext.destination);

          // Schedule playback for smooth streaming
          if (nextPlayTime < audioContext.currentTime) {
            nextPlayTime = audioContext.currentTime;
          }

          console.log("‚ñ∂Ô∏è Playing audio at time:", nextPlayTime);
          source.start(nextPlayTime);
          nextPlayTime += audioBuffer.duration;

          console.log(
            "‚úÖ Audio scheduled successfully, next play time:",
            nextPlayTime
          );
        } catch (error) {
          console.error("‚ùå Error playing audio:", error);
          addMessage(`‚ùå Audio playback error: ${error.message}`, "error");
        }
      }

      // WebRTC Connection Functions
      async function connectWebRTC() {
        try {
          const baseUrl = webrtcUrlInput.value.trim();
          if (!baseUrl) {
            addMessage("Please enter a valid server URL", "error");
            return;
          }

          addMessage(`üìû Connecting to WebRTC server: ${baseUrl}...`, "info");

          // Create WebRTC peer connection
          peerConnection = new RTCPeerConnection({
            iceServers: [{ urls: "stun:stun.l.google.com:19302" }],
          });

          // Get microphone access
          audioStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
          });

          // Add audio track to peer connection
          audioStream.getTracks().forEach((track) => {
            peerConnection.addTrack(track, audioStream);
            addMessage(`üé§ Audio track added: ${track.kind}`, "info");
          });

          // Handle ICE candidates
          peerConnection.onicecandidate = (event) => {
            if (event.candidate) {
              console.log("ICE candidate:", event.candidate);
            }
          };

          // Handle connection state changes
          peerConnection.onconnectionstatechange = () => {
            addMessage(
              `üì° WebRTC state: ${peerConnection.connectionState}`,
              "info"
            );
            if (peerConnection.connectionState === "connected") {
              updateConnectionStatus("Connected (WebRTC)", true);
              activeProtocol.textContent = "WebRTC";
            } else if (
              peerConnection.connectionState === "failed" ||
              peerConnection.connectionState === "closed"
            ) {
              updateConnectionStatus("Disconnected", false);
              activeProtocol.textContent = "None";
            }
          };

          // Create offer
          const offer = await peerConnection.createOffer();
          await peerConnection.setLocalDescription(offer);

          // Send offer to server
          const response = await fetch(`${baseUrl}/webrtc/offer`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              sdp: peerConnection.localDescription.sdp,
              type: peerConnection.localDescription.type,
            }),
          });

          if (!response.ok) {
            throw new Error(`Server error: ${response.status}`);
          }

          const answer = await response.json();
          await peerConnection.setRemoteDescription(
            new RTCSessionDescription(answer)
          );

          // Get session ID from response
          const sessionId = answer.session_id;
          if (!sessionId) {
            throw new Error("No session ID received from server");
          }

          addMessage(`üîë Session ID: ${sessionId.substring(0, 8)}...`, "info");

          // Connect WebSocket for signaling with session ID
          const wsUrl =
            baseUrl.replace("http://", "ws://").replace("https://", "wss://") +
            "/ws/webrtc/" +
            sessionId;
          webrtcSignalingWs = new WebSocket(wsUrl);

          webrtcSignalingWs.onopen = () => {
            addMessage("‚úÖ WebRTC signaling channel connected", "success");

            // Auto-start listening for WebRTC (audio flows automatically)
            isRecording = true;
            startTime = Date.now();
            chunksCount = 0;
            bytesCount = 0;

            transcriptBox.style.display = "block";
            transcriptContent.textContent = "Waiting for speech...";
            llmResponseBox.style.display = "none";
            updateRecordingStatus("Listening (WebRTC)...", true);
            addMessage("üé§ WebRTC audio streaming active - speak now!", "info");

            // Start duration counter
            durationInterval = setInterval(updateStats, 1000);

            // Initialize audio context
            initAudioContext();
            if (audioContext) {
              nextPlayTime = audioContext.currentTime;
            }
          };

          webrtcSignalingWs.onmessage = handleWebSocketMessage;

          webrtcSignalingWs.onerror = (error) => {
            addMessage("‚ùå WebRTC signaling error", "error");
            console.error("WebRTC signaling error:", error);
          };

          webrtcSignalingWs.onclose = () => {
            addMessage("üîå WebRTC signaling disconnected", "warning");
          };

          updateConnectionStatus("Connected (WebRTC)", true);
          activeProtocol.textContent = "WebRTC";
          connectBtn.disabled = true;
          disconnectBtn.disabled = false;
          startBtn.disabled = false;
          protocolSelect.disabled = true;
          webrtcUrlInput.disabled = true;

          addMessage("‚úÖ WebRTC connection established!", "success");
        } catch (error) {
          addMessage(`‚ùå WebRTC connection failed: ${error.message}`, "error");
          console.error("WebRTC error:", error);
          updateConnectionStatus("Error", false);

          // Cleanup
          if (peerConnection) {
            peerConnection.close();
            peerConnection = null;
          }
          if (audioStream) {
            audioStream.getTracks().forEach((track) => track.stop());
            audioStream = null;
          }
        }
      }

      function disconnectWebRTC() {
        if (peerConnection) {
          peerConnection.close();
          peerConnection = null;
          addMessage("WebRTC peer connection closed", "info");
        }
        if (webrtcSignalingWs) {
          webrtcSignalingWs.close();
          webrtcSignalingWs = null;
        }
        if (audioStream) {
          audioStream.getTracks().forEach((track) => track.stop());
          audioStream = null;
        }
        updateConnectionStatus("Disconnected", false);
        activeProtocol.textContent = "None";
        connectBtn.disabled = false;
        disconnectBtn.disabled = true;
        startBtn.disabled = true;
        protocolSelect.disabled = false;
        webrtcUrlInput.disabled = false;
      }

      // Add message to log
      function addMessage(message, type = "info") {
        const messageEl = document.createElement("div");
        messageEl.className = `message msg-${type}`;

        const timestamp = new Date().toLocaleTimeString();
        messageEl.innerHTML = `<span class="timestamp">[${timestamp}]</span>${message}`;

        messagesDiv.appendChild(messageEl);
        messagesDiv.scrollTop = messagesDiv.scrollHeight;

        // Keep only last 50 messages
        if (messagesDiv.children.length > 50) {
          messagesDiv.removeChild(messagesDiv.firstChild);
        }
      }

      // Update connection status
      function updateConnectionStatus(status, connected) {
        connectionStatus.textContent = status;
        connectionStatus.className = `status-value ${
          connected ? "status-connected" : "status-disconnected"
        }`;
      }

      // Update recording status
      function updateRecordingStatus(status, recording) {
        recordingStatus.textContent = status;
        recordingStatus.className = `status-value ${
          recording ? "status-recording" : ""
        }`;
      }

      // Update stats
      function updateStats() {
        chunksCountEl.textContent = chunksCount;
        bytesCountEl.textContent = bytesCount.toLocaleString();

        if (startTime && isRecording) {
          const duration = Math.floor((Date.now() - startTime) / 1000);
          durationEl.textContent = `${duration}s`;
        }
      }

      // Shared WebSocket message handler
      function handleWebSocketMessage(event) {
        const message = event.data;

        // Debug: log all messages
        console.log("üì® WebSocket message received:", message);

        // Handle INTERIM transcripts (user is speaking)
        if (message.startsWith("INTERIM: ")) {
          updateProcessFlow("listen");
          const transcript = message.replace("INTERIM: ", "");
          transcriptBox.classList.remove("hidden");
          transcriptContent.textContent = transcript + "...";
          transcriptContent.classList.add("thinking");
        }
        // Handle USER_SAID (final transcript before LLM processing)
        else if (message.startsWith("USER_SAID: ")) {
          const userMessage = message.replace("USER_SAID: ", "");

          // Show final user message
          transcriptBox.classList.remove("hidden");
          transcriptContent.textContent = userMessage;
          transcriptContent.classList.remove("thinking");

          // Show LLM response box and reset
          llmResponseBox.classList.remove("hidden");
          llmResponseContent.textContent = "ü§î Thinking...";
          llmResponseContent.classList.add("thinking");

          updateProcessFlow("think");
          addMessage(`üë§ You: ${userMessage}`, "info");
        }
        // Handle LLM_RESPONSE (streaming response chunks)
        else if (message.startsWith("LLM_RESPONSE: ")) {
          const chunk = message.replace("LLM_RESPONSE: ", "");

          // Append to LLM response (streaming)
          if (llmResponseContent.textContent === "ü§î Thinking...") {
            llmResponseContent.textContent = chunk;
            llmResponseContent.classList.remove("thinking");
          } else {
            llmResponseContent.textContent += chunk;
          }
          updateProcessFlow("think");
        }
        // Handle LLM_DONE (response complete)
        else if (message === "LLM_DONE") {
          const fullResponse = llmResponseContent.textContent;
          addMessage(`ü§ñ AI: ${fullResponse}`, "success");
        }
        // Handle TTS_START (text-to-speech starting)
        else if (message === "TTS_START") {
          updateProcessFlow("speak");
          audioStatus.textContent = "üîä Speaking...";
          audioStatus.style.display = "inline-flex";
          initAudioContext();

          // Clear previous audio chunks
          allAudioChunks = [];

          // Resume audio context on user interaction (browser security requirement)
          if (audioContext && audioContext.state === "suspended") {
            audioContext.resume().then(() => {
              console.log("‚úÖ Audio context resumed");
              addMessage("üîä AI is speaking...", "info");
            });
          } else {
            addMessage("üîä AI is speaking...", "info");
          }

          console.log(
            "üé§ TTS started, audio context state:",
            audioContext?.state
          );
        }
        // Handle TTS_AUDIO (audio chunks)
        else if (message.startsWith("TTS_AUDIO: ")) {
          const audioData = message.replace("TTS_AUDIO: ", "");
          console.log("üì• Received TTS audio chunk, size:", audioData.length);

          // Store chunk
          allAudioChunks.push(audioData);

          // Play chunk immediately
          playAudioChunk(audioData);
        }
        // Handle TTS_DONE (speech complete)
        else if (message === "TTS_DONE") {
          console.log(
            "‚úÖ TTS_DONE received, total chunks:",
            allAudioChunks.length
          );
          audioStatus.textContent = "‚úÖ Done";
          setTimeout(() => {
            audioStatus.style.display = "none";
          }, 2000);
          addMessage(
            `‚úÖ AI finished speaking (${allAudioChunks.length} chunks)`,
            "success"
          );
          updateProcessFlow("listen");
        }
        // Handle LATENCY metrics
        else if (message.startsWith("LATENCY: ")) {
          const latencyJson = message.replace("LATENCY: ", "");
          try {
            const metrics = JSON.parse(latencyJson);
            updateLatencyMetrics(metrics);
            console.log("üìä Latency metrics:", metrics);
          } catch (e) {
            console.error("Error parsing latency metrics:", e);
          }
        }
        // Handle STATS (WebRTC frame stats)
        else if (message.startsWith("STATS: ")) {
          const statsJson = message.replace("STATS: ", "");
          try {
            const stats = JSON.parse(statsJson);
            chunksCount = stats.chunks || chunksCount;
            bytesCount = stats.bytes || bytesCount;
            updateStats();
          } catch (e) {
            console.error("Error parsing stats:", e);
          }
        }
        // Regular server messages
        else {
          addMessage(`üì• Server: ${message}`, "success");
        }
      }

      // Connect to WebSocket
      async function connectWebSocket() {
        const wsUrl = wsUrlInput.value.trim();

        if (!wsUrl) {
          addMessage("Please enter a valid WebSocket URL", "error");
          return;
        }

        try {
          updateProcessFlow("connect");
          addMessage(`Connecting to ${wsUrl}...`, "info");
          websocket = new WebSocket(wsUrl);

          websocket.onopen = () => {
            addMessage("‚úÖ WebSocket connected successfully!", "success");
            updateConnectionStatus("Connected", true);
            activeProtocol.textContent = "WebSocket";
            connectBtn.disabled = true;
            disconnectBtn.disabled = false;
            startBtn.disabled = false;
            protocolSelect.disabled = true;
            wsUrlInput.disabled = true;
            updateProcessFlow("listen");
          };

          websocket.onmessage = handleWebSocketMessage;

          websocket.onerror = (error) => {
            addMessage(`‚ùå WebSocket error occurred`, "error");
            console.error("WebSocket error:", error);
          };

          websocket.onclose = () => {
            addMessage("üîå WebSocket disconnected", "warning");
            updateConnectionStatus("Disconnected", false);
            activeProtocol.textContent = "None";
            connectBtn.disabled = false;
            disconnectBtn.disabled = true;
            startBtn.disabled = true;
            stopBtn.disabled = true;
            protocolSelect.disabled = false;
            wsUrlInput.disabled = false;
            updateProcessFlow("none");

            if (isRecording) {
              stopRecording();
            }
          };
        } catch (error) {
          addMessage(`‚ùå Connection failed: ${error.message}`, "error");
          updateConnectionStatus("Error", false);
          updateProcessFlow("none");
        }
      }

      // Disconnect from WebSocket
      function disconnectWebSocket() {
        if (websocket) {
          if (isRecording) {
            stopRecording();
          }
          websocket.close();
          websocket = null;
        }
        updateConnectionStatus("Disconnected", false);
        activeProtocol.textContent = "None";
        connectBtn.disabled = false;
        disconnectBtn.disabled = true;
        startBtn.disabled = true;
        protocolSelect.disabled = false;
        wsUrlInput.disabled = false;
        updateProcessFlow("none");
      }

      // Universal connect function
      async function connect() {
        if (currentProtocol === "websocket") {
          await connectWebSocket();
        } else {
          await connectWebRTC();
        }
      }

      // Universal disconnect function
      function disconnect() {
        if (currentProtocol === "websocket") {
          disconnectWebSocket();
        } else {
          disconnectWebRTC();
        }
      }

      // Start recording
      async function startRecording() {
        // Check connection based on protocol
        if (currentProtocol === "websocket") {
          if (!websocket || websocket.readyState !== WebSocket.OPEN) {
            addMessage("‚ùå WebSocket not connected", "error");
            return;
          }
        } else if (currentProtocol === "webrtc") {
          if (
            !peerConnection ||
            peerConnection.connectionState !== "connected"
          ) {
            addMessage("‚ùå WebRTC not connected", "error");
            return;
          }
        }

        try {
          if (currentProtocol === "webrtc") {
            // For WebRTC, audio is already streaming, just update UI
            isRecording = true;
            startTime = Date.now();
            chunksCount = 0;
            bytesCount = 0;

            updateRecordingStatus("Streaming", true);
            addMessage("üî¥ WebRTC audio streaming active", "success");

            // Show transcript box and reset content
            transcriptBox.classList.remove("hidden");
            transcriptContent.textContent = "Waiting for speech...";
            transcriptContent.classList.add("thinking");
            llmResponseBox.classList.add("hidden");
            llmResponseContent.textContent = "ü§î Thinking...";
            audioStatus.style.display = "none";
            latencyCard.style.display = "none";

            // Reset audio playback
            if (audioContext) {
              nextPlayTime = audioContext.currentTime;
            }

            startBtn.disabled = true;
            stopBtn.disabled = false;
            disconnectBtn.disabled = true;
            updateProcessFlow("listen");

            // Update duration every second
            durationInterval = setInterval(updateStats, 1000);
          } else {
            // WebSocket mode - use MediaRecorder
            addMessage("üé§ Requesting microphone access...", "info");
            audioStream = await navigator.mediaDevices.getUserMedia({
              audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
              },
            });

            addMessage("‚úÖ Microphone access granted", "success");

            // Create MediaRecorder
            const mimeType = MediaRecorder.isTypeSupported("audio/webm")
              ? "audio/webm"
              : "audio/ogg";

            mediaRecorder = new MediaRecorder(audioStream, {
              mimeType: mimeType,
              audioBitsPerSecond: 128000,
            });

            addMessage(`üìä Using codec: ${mimeType}`, "info");

            mediaRecorder.ondataavailable = (event) => {
              if (
                event.data.size > 0 &&
                websocket.readyState === WebSocket.OPEN
              ) {
                websocket.send(event.data);
                chunksCount++;
                bytesCount += event.data.size;
                updateStats();
              }
            };

            mediaRecorder.onstart = () => {
              isRecording = true;
              startTime = Date.now();
              chunksCount = 0;
              bytesCount = 0;

              updateRecordingStatus("Recording", true);
              addMessage("üî¥ Recording started", "success");
              websocket.send("start_recording");

              // Show transcript box and reset content
              transcriptBox.classList.remove("hidden");
              transcriptContent.textContent = "Waiting for speech...";
              transcriptContent.classList.add("thinking");
              llmResponseBox.classList.add("hidden");
              llmResponseContent.textContent = "ü§î Thinking...";
              audioStatus.style.display = "none";
              latencyCard.style.display = "none";

              // Reset audio playback
              if (audioContext) {
                nextPlayTime = audioContext.currentTime;
              }

              startBtn.disabled = true;
              stopBtn.disabled = false;
              disconnectBtn.disabled = true;
              updateProcessFlow("listen");

              // Update duration every second
              durationInterval = setInterval(updateStats, 1000);
            };

            mediaRecorder.onstop = () => {
              isRecording = false;
              updateRecordingStatus("Idle", false);
              addMessage("‚èπÔ∏è Recording stopped", "info");

              if (websocket && websocket.readyState === WebSocket.OPEN) {
                websocket.send("stop_recording");
              }

              startBtn.disabled = false;
              stopBtn.disabled = true;
              disconnectBtn.disabled = false;

              if (durationInterval) {
                clearInterval(durationInterval);
                durationInterval = null;
              }

              // Stop all tracks
              if (audioStream) {
                audioStream.getTracks().forEach((track) => track.stop());
                audioStream = null;
              }
            };

            mediaRecorder.onerror = (error) => {
              addMessage(`‚ùå MediaRecorder error: ${error.name}`, "error");
              console.error("MediaRecorder error:", error);
            };

            // Start recording with 250ms chunks
            mediaRecorder.start(250);
          }
        } catch (error) {
          addMessage(`‚ùå Error starting recording: ${error.message}`, "error");
          console.error("Recording error:", error);
          updateRecordingStatus("Error", false);
        }
      }

      // Stop recording
      function stopRecording() {
        if (currentProtocol === "webrtc") {
          // For WebRTC, just update UI state
          isRecording = false;
          updateRecordingStatus("Idle", false);
          addMessage("‚èπÔ∏è WebRTC streaming paused", "info");

          startBtn.disabled = false;
          stopBtn.disabled = true;
          disconnectBtn.disabled = false;

          if (durationInterval) {
            clearInterval(durationInterval);
            durationInterval = null;
          }
        } else {
          // WebSocket - stop MediaRecorder
          if (mediaRecorder && mediaRecorder.state !== "inactive") {
            mediaRecorder.stop();
          }
        }
      }

      // Event listeners
      // Test audio playback function
      function testAudio() {
        try {
          initAudioContext();

          // Resume if needed
          if (audioContext.state === "suspended") {
            audioContext.resume();
          }

          console.log("üîä Testing audio - generating beep...");
          addMessage("üîä Testing audio playback...", "info");

          // Generate a simple test beep
          const duration = 0.5;
          const frequency = 440; // A4 note
          const sampleRate = audioContext.sampleRate;
          const numSamples = duration * sampleRate;

          const buffer = audioContext.createBuffer(1, numSamples, sampleRate);
          const channelData = buffer.getChannelData(0);

          // Generate sine wave
          for (let i = 0; i < numSamples; i++) {
            channelData[i] =
              Math.sin((2 * Math.PI * frequency * i) / sampleRate) * 0.3;
          }

          // Play it
          const source = audioContext.createBufferSource();
          source.buffer = buffer;
          source.connect(audioContext.destination);
          source.start(audioContext.currentTime);

          console.log(
            "‚úÖ Test beep played! If you heard it, audio is working."
          );
          addMessage("‚úÖ Test beep played! Did you hear it?", "success");
        } catch (error) {
          console.error("‚ùå Audio test failed:", error);
          addMessage(`‚ùå Audio test failed: ${error.message}`, "error");
        }
      }

      // Event listeners
      connectBtn.addEventListener("click", connect);
      disconnectBtn.addEventListener("click", disconnect);
      startBtn.addEventListener("click", startRecording);
      stopBtn.addEventListener("click", stopRecording);
      testAudioBtn.addEventListener("click", testAudio);

      // Clean up on page unload
      window.addEventListener("beforeunload", () => {
        if (isRecording) {
          stopRecording();
        }
        if (websocket) {
          websocket.close();
        }
      });

      // Initial message
      addMessage("üëã Welcome! Connect to start streaming audio", "info");
    </script>
  </body>
</html>
